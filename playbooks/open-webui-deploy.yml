---
# Playbook: Deploy Open WebUI
# Description: Deploy Open WebUI web interface for LLM (llama.cpp with Intel Arc Pro B60)
# Prerequisites:
#   - VM 201 (ipex-llm) with llama.cpp server running on port 8000
#   - LXC 200 "Containers" with Docker and Traefik
#   - DNS record for llm.viljo.se pointing to public IP

- name: Deploy Open WebUI
  hosts: proxmox_admin
  gather_facts: false

  vars:
    lxc_id: 200
    service_name: open-webui
    service_subdomain: llm
    service_domain: "{{ service_subdomain }}.{{ public_domain }}"
    # llama.cpp server with OpenAI-compatible API (Intel Arc Pro B60 GPU)
    openai_api_url: "http://172.31.31.201:8000/v1"
    openai_api_key: "sk-local-connector"
    deploy_path: "/opt/docker-stack/open-webui"
    webui_secret_key: "{{ vault_open_webui_secret_key | default('change-me-to-random-secret') }}"

  pre_tasks:
    - name: Display deployment plan
      ansible.builtin.debug:
        msg:
          - "=========================================="
          - "Open WebUI Deployment"
          - "=========================================="
          - "This playbook will deploy:"
          - "- Open WebUI container (ghcr.io/open-webui/open-webui:main)"
          - "- Web interface for LLM (llama.cpp on Intel Arc Pro B60)"
          - ""
          - "Configuration:"
          - "  LXC: {{ lxc_id }} (Containers)"
          - "  Domain: {{ service_domain }}"
          - "  LLM Backend: llama.cpp (OpenAI-compatible API)"
          - "  LLM API URL: {{ openai_api_url }}"
          - "  GPU: Intel Arc Pro B60 24GB (VM 201)"
          - "  Model: Qwen2.5-Coder-7B-Instruct (Q4_K_M)"
          - "  Deploy Path: {{ deploy_path }}"
          - "  Authentication: Disabled (OAuth2-proxy handles auth)"
          - ""
          - "IMPORTANT:"
          - "  This playbook deploys Open WebUI WITHOUT authentication."
          - "  To protect with OAuth, also deploy:"
          - "    ansible-playbook playbooks/oauth2-proxy-llm-deploy.yml"
          - ""
          - "Traefik Integration:"
          - "  - Automatic HTTPS via Let's Encrypt"
          - "  - Routing: Host(`{{ service_domain }}`)"
          - "=========================================="

  tasks:
    - name: Create deployment directory
      ansible.builtin.command: >
        pct exec {{ lxc_id }} -- mkdir -p {{ deploy_path }}/data
      changed_when: true

    - name: Create docker-compose.yml
      ansible.builtin.copy:
        dest: /tmp/open-webui-compose.yml
        mode: '0644'
        content: |
          version: '3.8'

          services:
            open-webui:
              image: ghcr.io/open-webui/open-webui:main
              container_name: open-webui
              restart: unless-stopped

              environment:
                # llama.cpp OpenAI-compatible API (Intel Arc Pro B60 GPU)
                - OPENAI_API_BASE_URL={{ openai_api_url }}
                - OPENAI_API_KEY={{ openai_api_key }}
                # Disable Ollama (using OpenAI-compatible API only)
                - ENABLE_OLLAMA_API=false
                # Disable Open WebUI auth - OAuth2-proxy handles authentication
                - WEBUI_AUTH=false
                - WEBUI_NAME=Viljo LLM
                - WEBUI_SECRET_KEY={{ webui_secret_key }}
                - DATA_DIR=/app/backend/data
                # Enable API key authentication for API endpoints
                - ENABLE_API_KEY=true
                # Disable local signup and login form
                - ENABLE_SIGNUP=false
                - ENABLE_LOGIN_FORM=false
                # Default model (llama.cpp GGUF model path)
                - DEFAULT_MODELS=/opt/llama.cpp/models/qwen2.5-coder-7b-instruct-q4_k_m.gguf
                # Web Search Configuration - Using DuckDuckGo (no API key needed)
                - ENABLE_RAG_WEB_SEARCH=true
                - RAG_WEB_SEARCH_ENGINE=duckduckgo
                - RAG_WEB_SEARCH_RESULT_COUNT=5
                - RAG_WEB_SEARCH_CONCURRENT_REQUESTS=10

              volumes:
                - {{ deploy_path }}/data:/app/backend/data

              labels:
                # Disable Traefik - OAuth2-proxy handles routing
                - "traefik.enable=false"

                # Watchtower
                - "com.centurylinklabs.watchtower.enable=true"

              networks:
                - backend

              deploy:
                resources:
                  limits:
                    cpus: '2.0'
                    memory: 2G

          networks:
            backend:
              external: true

    - name: Copy docker-compose.yml to LXC
      ansible.builtin.command: >
        pct push {{ lxc_id }} /tmp/open-webui-compose.yml {{ deploy_path }}/docker-compose.yml
      changed_when: true

    - name: Deploy Open WebUI container
      ansible.builtin.command: >
        pct exec {{ lxc_id }} -- bash -c 'cd {{ deploy_path }} && docker compose up -d'
      changed_when: true

    - name: Wait for container to start
      ansible.builtin.wait_for:
        timeout: 30
      delegate_to: localhost

    - name: Check container status
      ansible.builtin.command: >
        pct exec {{ lxc_id }} -- docker ps --filter name=open-webui --format "{{ '{{' }}.Status{{ '}}' }}"
      register: container_status
      changed_when: false

    - name: Verify LLM API connectivity from container
      ansible.builtin.command: >
        pct exec {{ lxc_id }} -- docker exec open-webui curl -sf {{ openai_api_url }}/models -H "Authorization: Bearer {{ openai_api_key }}"
      register: llm_api_check
      changed_when: false
      failed_when: false

    - name: Check HTTPS endpoint
      ansible.builtin.uri:
        url: "https://{{ service_domain }}"
        method: GET
        validate_certs: true
        status_code: 200
      register: https_check
      delegate_to: localhost
      failed_when: false
      retries: 6
      delay: 10

  post_tasks:
    - name: Display deployment summary
      ansible.builtin.debug:
        msg:
          - "=========================================="
          - "Open WebUI Deployment Complete"
          - "=========================================="
          - ""
          - "✅ Container Deployed:"
          - "  Status: {{ container_status.stdout }}"
          - "  Image: ghcr.io/open-webui/open-webui:main"
          - "  Port: 8080 (internal)"
          - ""
          - "✅ LLM Backend Connection:"
          - "  Type: llama.cpp (OpenAI-compatible API)"
          - "  API URL: {{ openai_api_url }}"
          - "  GPU: Intel Arc Pro B60 24GB (VM 201)"
          - "  Model: Qwen2.5-Coder-7B-Instruct (Q4_K_M)"
          - "  Status: {{ 'Connected' if llm_api_check.rc == 0 else 'Failed - check VM 201 llama-server' }}"
          - ""
          - "✅ HTTPS Access:"
          - "  URL: https://{{ service_domain }}"
          - "  Status: {{ 'Available' if https_check.status == 200 else 'Pending (may need DNS propagation)' }}"
          - "  SSL: Let's Encrypt"
          - ""
          - "=========================================="
          - "NEXT STEPS"
          - "=========================================="
          - ""
          - "1. Access Open WebUI:"
          - "   https://{{ service_domain }}"
          - ""
          - "2. Start chatting with the Qwen2.5-Coder model:"
          - "   - Model is pre-loaded on Intel Arc Pro B60 GPU"
          - ""
          - "3. Check container logs if needed:"
          - "   pct exec {{ lxc_id }} -- docker logs open-webui"
          - ""
          - "4. Check LLM server logs:"
          - "   ssh root@172.31.31.201 journalctl -u llama-server -f"
          - ""
          - "=========================================="

    - name: Final success message
      ansible.builtin.debug:
        msg:
          - "=========================================="
          - "SUCCESS! Open WebUI is Running"
          - "=========================================="
          - "Access at: https://{{ service_domain }}"
          - "LLM Backend: llama.cpp on Intel Arc Pro B60 (VM 201)"
          - "Model: Qwen2.5-Coder-7B-Instruct"
          - "=========================================="
